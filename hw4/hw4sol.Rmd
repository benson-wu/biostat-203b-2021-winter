---
title: "Biostat 203B Homework 4 (Draft)"
subtitle: Due Mar 12 @ 11:59PM
output:
  html_document:
    toc: true
    toc_depth: 4
  # ioslides_presentation: default
---

```{r, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```
                      
Display machine information:
```{r}
sessionInfo()
```
Load database libraries and the tidyverse frontend:
```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(lubridate)
library(miceRanger)
library(stringr)
library(mice)
library(ddpcr)
library(data.table)
library(caret)
library(healthcareai)
library(glmnet)
library(randomForest)
```

Change directory depending on environment 
```{r}
os <- sessionInfo()$running

#Generate path for mimic data
if (str_detect(os, "Linux")) {
  working_path <- "/home/buwenson/biostat-203b-2021-winter"
} else if (str_detect(os, "macOS")) {
  working_path <- 
    "/Users/bensonwu/Documents/UCLA/2020-2021/Winter 2021/BIOSTAT_203B/biostat-203b-2021-winter"
} else {
  working_path <- "C:/Users/benson.wu/Documents/biostat-203b-2021-winter-develop/"
}
```


## Q1. Missing data

Through the Shiny app developed in HW3, we observe abundant missing values in the MIMIC-IV ICU cohort we created. In this question, we use multiple imputation to obtain a data set without missing values.

### Part 0

0. Read following tutorials on the R package miceRanger for imputation: <https://github.com/farrellday/miceRanger>, <https://cran.r-project.org/web/packages/miceRanger/vignettes/miceAlgorithm.html>.

    A more thorough book treatment of the practical imputation strategies is the book [*_Flexible Imputation of Missing Data_*](https://stefvanbuuren.name/fimd/) by Stef van Buuren. 

### Part 1

1. Explain the jargon MCAR, MAR, and MNAR.

**Q1.1 Solution**

**MCAR** stands for missing completely at random. This means that the probability of being missing is the same for all cases. 

**MAR** stands for missing at random. This means that the probability of being missing is the same only within groups defined by the observed data. 

**MNAR** stands for missing not at random. This means that the probability of being missing varies for reasons that are unknown to us. 

### Part 2

2. Explain in a couple of sentences how the Multiple Imputation by Chained Equations (MICE) work.

**Q1.2 Solution**

Multiple Imputation by Chained Equations is a robust, informative method of dealing with missing data in datasets. The procedure ‘fills in’ (imputes) missing data in a dataset through an iterative series of predictive models. In each iteration, each specified variable in the dataset is imputed using the other variables in the dataset. These iterations should be run until it appears that convergence has been met.

### Part 3

3. Perform a data quality check of the ICU stays data. Discard variables with substantial missingness, say >5000 `NA`s. Replace apparent data entry errors by `NA`s.

**Q1.3 Solution**

We are interested in gender, age, marital status, ethnicity, first lab measurements during ICU stay, and first vital measurements during ICU stay. 

First, let's drop all variables that we will not use
```{r}
#Read in data set
icu_cohort <- readRDS(str_c(working_path, "/hw3/mimiciv_shiny/icu_cohort.rds"))

keeps<-c("subject_id", "gender", "age_at_adm", "marital_status", "ethnicity", 
         "bicarbonate", "calcium", "chloride", "creatinine", "glucose", 
         "magnesium", "potassium", "sodium", "hematocrit", "wbc", "lactate",
         "heart_rate", "non_invasive_blood_pressure_systolic", 
         "non_invasive_blood_pressure_mean", "respiratory_rate",
         "temperature_fahrenheit", "arterial_blood_pressure_systolic", 
         "arterial_blood_pressure_mean", "died_within_30_days")
icu_cohort_filtered <- icu_cohort[, keeps, drop=FALSE]
```


Next, let's recode erroneous values to `NA` and clean up data. 

```{r}
#Marital status
icu_cohort_filtered$marital_status[icu_cohort_filtered$marital_status == ""] <-
  NA
icu_cohort_filtered$marital_status <- factor(icu_cohort_filtered$marital_status)

#Ethnicity: "UNKNOWN" and "UNABLE TO OBTAIN" were recoded to NA
icu_cohort_filtered$ethnicity[icu_cohort_filtered$ethnicity == "UNKNOWN"] <- NA
icu_cohort_filtered$ethnicity[icu_cohort_filtered$ethnicity == "UNABLE TO OBTAIN"] <- NA
icu_cohort_filtered$ethnicity <- factor(icu_cohort_filtered$ethnicity)

#Recode gender to make the values numerical
icu_cohort_filtered$gender<-recode(icu_cohort_filtered$gender, `F` = 0, `M` = 1)
icu_cohort_filtered$gender <- factor(icu_cohort_filtered$gender)

#Factorize death within 30 days
icu_cohort_filtered$died_within_30_days <- factor(
  icu_cohort_filtered$died_within_30_days)
```

Recode extreme values to NA. 
```{r}
#Heart rate
tail(sort(icu_cohort_filtered$heart_rate),5)  
icu_cohort_filtered$heart_rate[icu_cohort_filtered$heart_rate == 941] <- NA

#Non-invasive systolic blood pressure
tail(sort(icu_cohort_filtered$non_invasive_blood_pressure_systolic),5) 
icu_cohort_filtered$non_invasive_blood_pressure_systolic[icu_cohort_filtered$non_invasive_blood_pressure_systolic == 12262] <- NA

#Non-invasive mean blood pressure
tail(sort(icu_cohort_filtered$non_invasive_blood_pressure_mean),10) 
max<-c(936, 6116, 120130, 140119)
icu_cohort_filtered$non_invasive_blood_pressure_mean[icu_cohort_filtered$non_invasive_blood_pressure_mean %in% max] <- NA

#Respiratory rate 
tail(sort(icu_cohort_filtered$respiratory_rate),5) #No extreme outliers 

#Temperature 
tail(sort(icu_cohort_filtered$temperature_fahrenheit),5) #No extreme outliers 

#Arterial systolic blood pressure 
tail(sort(icu_cohort_filtered$arterial_blood_pressure_systolic),10) 
icu_cohort_filtered$non_invasive_blood_pressure_mean[icu_cohort_filtered$non_invasive_blood_pressure_mean == 703] <- NA

#Arterial mean blood pressure 
head(sort(icu_cohort_filtered$arterial_blood_pressure_mean),100) 
tail(sort(icu_cohort_filtered$arterial_blood_pressure_mean),100) #Not sure yet, come back to this 

#Nothing to change for lab measurements



```


Now let's filter out variables that have over 5000 missing values. 

```{r}
#Filter out variables with over 5000 missingness. This code gets the percentage
#of NAs and only keeps variables that have less than (5000/#number of rows in the data frame)% 
#missingness

icu_cohort_filtered <- 
  icu_cohort_filtered[, which(
    colMeans(is.na(icu_cohort_filtered)) < 5000/nrow(icu_cohort))]
```

### Part 4

4. Impute missing values by `miceRanger` (request $m=3$ datasets). This step is very computational intensive. Make sure to save the imputation results as a file.

**Q1.4 Solution**

The output of miceRanger was saved to a .RData file to avoid running miceRanger again. 
```{r cache.lazy=FALSE}
if(file.exists(str_c(working_path, "hw4/miceObj1.RData"))){
  load(str_c(working_path, "hw4/miceObj1.RData"))
} else{
  seqTime <- system.time(
    miceObj <- miceRanger(icu_cohort_filtered, m=3,
                          returnModels = TRUE, verbose=FALSE, 
                          max.depth=20)
  )
  print(seqTime)
}
```


### Part 5

5. Make imputation diagnostic plots and explain what they mean.

**Q1.5 Solution**

```{r}
# https://cran.r-project.org/web/packages/miceRanger/vignettes/diagnosticPlotting.html

#Distribution of imputed values 
plotDistributions(miceObj,vars='allNumeric')

#Convergence of correlation 
plotCorrelations(miceObj,vars='allNumeric')

#Center and Dispersion Convergence
plotVarConvergence(miceObj,vars='allNumeric')
```

### Part 6

6. Obtain a complete data set by averaging the 3 imputed data sets.

**Q1.6 Solution**

We will use `quiet` to supress the output to speed up the dataframe combining process.

```{r}
#Obtain imputed datasets
quiet(imputedList <- completeData(miceObj))

#Convert categorical variables to numeric to find means 
quiet(imputedList$Dataset_1$marital_status <- 
  unclass(imputedList$Dataset_1$marital_status))

quiet(imputedList$Dataset_2$marital_status <- 
  unclass(imputedList$Dataset_2$marital_status))

quiet(imputedList$Dataset_3$marital_status <- 
  unclass(imputedList$Dataset_3$marital_status))

quiet(imputedList$Dataset_1$gender <- 
  as.numeric(imputedList$Dataset_1$gender))

quiet(imputedList$Dataset_2$gender <- 
  as.numeric(imputedList$Dataset_2$gender))

quiet(imputedList$Dataset_3$gender <- 
  as.numeric(imputedList$Dataset_3$gender))

quiet(imputedList$Dataset_1$died_within_30_days <- 
  as.numeric(imputedList$Dataset_1$died_within_30_days))

quiet(imputedList$Dataset_2$died_within_30_days <- 
  as.numeric(imputedList$Dataset_2$died_within_30_days))

quiet(imputedList$Dataset_3$died_within_30_days <- 
  as.numeric(imputedList$Dataset_3$died_within_30_days))




#Average data sets
quiet(
  icu_cohort_filtered_imputed <- 
    rbindlist(list(imputedList$Dataset_1, 
                   imputedList$Dataset_2, 
                   imputedList$Dataset_3))[,lapply(.SD,mean), list(subject_id)]
  )

#Factorize gender and 30 day mortality status again
icu_cohort_filtered_imputed$gender<-recode(icu_cohort_filtered_imputed$gender, 
                                           `1` = 0, `2` = 1)
icu_cohort_filtered_imputed$gender <- factor(icu_cohort_filtered_imputed$gender)

#Factorize death within 30 days
icu_cohort_filtered_imputed$died_within_30_days<-recode(
  icu_cohort_filtered_imputed$died_within_30_days, `1` = 0, `2` = 1)

icu_cohort_filtered_imputed$died_within_30_days <- factor(
  icu_cohort_filtered_imputed$died_within_30_days)

#Remove subject_id variable now
icu_cohort_filtered_imputed <- subset(icu_cohort_filtered_imputed, select = -c(subject_id))
```



Remove unnecessary objects to clear environment memory. 
```{r}
rm(miceObj,imputedList, icu_cohort_filtered)
```

## Q2. Predicting 30-day mortality

Develop at least two analytic approaches for predicting the 30-day mortality of patients admitted to ICU using demographic information (gender, age, marital status, ethnicity), first lab measurements during ICU stay, and first vital measurements during ICU stay. For example, you can use (1) logistic regression (`glm()` function), (2) logistic regression with lasso penalty (glmnet package), (3) random forest (randomForest package), or (4) neural network.

### Part 1

1. Partition data into 80% training set and 20% test set. Stratify partitioning according the 30-day mortality status.

**Q2.1 Solution**

First let's tabulate the 30-day mortality status variable.
```{r}
table(icu_cohort_filtered_imputed$died_within_30_days)/
  nrow(icu_cohort_filtered_imputed)
```

We will stratify the partioning according to these prevalences of 30-day mortality status. We will use `split_train_test` from the `healthcareai` package to accomplish this. 

```{r}
partitioned_data<-split_train_test(d=icu_cohort_filtered_imputed, 
                                   outcome=died_within_30_days, 
                                   percent_train = 0.8, seed=1)
train <- partitioned_data$train
test <- partitioned_data$test
```

### Parts 2 & 3

2. Train the models using the training set.

3. Compare model prediction performance on the test set.

**Q2.2 and Q2.3 Solution**

#### Logistic regression with lasso penalty

We will use `glmnet()` to perform this analysis. First, we need to do some data preparation. We will use `model.matrix()` to create a matrix of predictors with categorical variables one-hot-encoded. Categorical variables need to be one-hot-encoded to work for `glmnet()`. Reference for my code can be found here: http://www.sthda.com/english/articles/36-classification-methods-essentials/149-penalized-logistic-regression-essentials-in-r-ridge-lasso-and-elastic-net/

```{r}
#Dummy code categorical predictor variables
x<-model.matrix(died_within_30_days~., train)[,-1]

#Assign outcome to y
y<-train$died_within_30_days
```


Now we can computed the penalized logistic regression. First we'll find the best lambda value to minimuze the cross_validation error. The lambda that minimizes the prediction error is $\lambda_{min}$ = 0.0002973709. However, [literature](https://stats.stackexchange.com/questions/138569/why-is-lambda-within-one-standard-error-from-the-minimum-is-a-recommended-valu) sugggests using the lambda within one standard error from the minimum as it gives the simplest model. The calculated lambda within one standard error is $\lambda_{1se}$ = 0.004846405
```{r}
set.seed(1)
cv.lasso <- cv.glmnet(x, y, alpha=1, family = "binomial")
plot(cv.lasso)
cv.lasso$lambda.min
cv.lasso$lambda.1se
```

&nbsp; 
&nbsp; 
&nbsp; 

**Lambda that minimizes prediction error** 

We first use the lambda value that minimizes the prediction error. 

```{r}
#Fit model on training data
model_min <- glmnet(x, y, family= "binomial", alpha=1, lambda=cv.lasso$lambda.min)
```

Now we can make predictions on the test data.
```{r}
x.test<-model.matrix(died_within_30_days~., test)[,-1]
probabilities_min <- model_min %>% predict (newx = x.test)
predicted.classes_min <- ifelse(probabilities_min > 0.5, 1, 0)
```

Let's assess model accurracy. 

```{r}
observed.classes <- test$died_within_30_days
mean(predicted.classes_min == observed.classes)
```

Using $\lambda_{min}$ gives an accuracy of 90.5%. 

&nbsp; 
&nbsp; 
&nbsp; 

**Lambda that is one standard error from the minimum**

We first use the lambda value that minimizes the prediction error. 

```{r}
#Fit model on training data
model_1se <- glmnet(x, y, family= "binomial", alpha=1, lambda=cv.lasso$lambda.1se)
```

Now we can make predictions on the test data.
```{r}
probabilities_1se <- model_1se %>% predict (newx = x.test)
predicted.classes_1se <- ifelse(probabilities_1se > 0.5, 1, 0)
```

Let's assess model accurracy. 

```{r}
mean(predicted.classes_1se == observed.classes)
```

Using $\lambda_{1se}$ gives an accuracy of 90.3%. 

&nbsp; 

Using $\lambda_{1se}$ gives a slightly less model accuracy compared to using $\lambda_{min}$. However, we see that the two models give different results for the values of the coefficients. The model using $\lambda_{1se}$ is less complex and does at least as good of a job as the more complex model with $\lambda_{min}$, so this may be the preferred model to avoid overfitting. 

```{r}
#Using the lambda that minimizes prediction error 
coef(model_min)

#Using the lambda that is 1 SE from the minimum 
coef(model_1se)
```

&nbsp; 
&nbsp; s
&nbsp; 
&nbsp; 

#### Random forest

Let's attempt to use random forests to classify patients to a 30-day mortality status. 

Note: 3/4 left off on trying to see if there's a way to grid search for the optimal parameters. Need to see how to interpret what I'm doing as well
```{r}
# Clear memory first

#Garbage collection to clear memory
gc()

rm(model_min, model_1se, probab)
```


```{r}
mortalitystatus_train <- randomForest(died_within_30_days~., data=train, 
                                      ntree=5000, importance=TRUE)

```